{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Efficient Fine-Tuning with LoRA (Tutorial)\n",
        "\n",
        "This notebook contains a brief example applying LoRA to fine-tune DistilBERT for sequence classification on limited hardware resources (v5e-1 TPU in Colab). The model weights and data are loaded using the HuggingFace framework, which also contains the Parameter-Efficient Fine-Tuning (PEFT) used for fine-tuning with LoRA.\n",
        "\n",
        "### What is LoRA?\n",
        "\n",
        "Low-Rank Adaptation (LoRA) is a fine-tuning technique used for Large Language Models (LLMs).\n",
        "\n",
        "LoRA seeks to reduce the enormous computational cost of finetuning, and to simplify the process of adapting a pre-trained LLM for various downstream tasks. This is achieved by:\n",
        "\n",
        "\n",
        "1.   Computing a low-rank adaptation of some weights (originally, the attention weights)\n",
        "\n",
        "2.   Training this low-rank adaptation during fine-tuning\n",
        "\n",
        "3.   Adding these weights back to the pre-trained LLM for inference\n",
        "\n",
        "### Why use LoRA?\n",
        "\n",
        "The pretrained model is preserved. This is preferable to retraining the entire model during fine-tuning for a couple of reasons:\n",
        "1.   Improved performance on downstream tasks (the model does not \"forget\" the general knowledge acquired during pretraining)\n",
        "2. Straightforward fine-tuning on other tasks that requires less memory (fine-tuned weights are low-rank; they can be saved separately and added on to the pre-trained \"base\" model as necerssary)\n",
        "\n",
        "LoRA also has advantages over previous fine-tuning methods, such as\n",
        "1. adapter layers (typically linear layers added to transformer module -> increase model depth -> slow down inference)\n",
        "2. and prefix tuning, in which prefix tokens are prepended to prompt -> smaller % input directly relevant to prompt -> potentially worse performance. Moreover, the authors of the LoRA paper note that this method is \"difficult to optimize\" and that its performance with respect to model size is difficult to predict.\n",
        "\n",
        "### Original LoRA Paper:\n",
        "\n",
        "PDF: https://arxiv.org/pdf/2106.09685\n"
      ],
      "metadata": {
        "id": "uYDI_7z94he3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "ypzF3lcSCYmy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DZs07z2_fRvE"
      },
      "outputs": [],
      "source": [
        "# useful huggingface packages for training transformers\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer,TrainingArguments, DataCollatorWithPadding\n",
        "\n",
        "# huggingface packages for loading data\n",
        "from datasets import load_from_disk\n",
        "\n",
        "# finetuning-specific packages\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "# for subsampling data\n",
        "import random\n",
        "\n",
        "# for loading checkpoints\n",
        "import os\n",
        "\n",
        "# used for inference - scroll to bottom for demo!!\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model\n",
        "\n",
        "This notebook uses DistilBERT, a smaller version of the BERT LLM trained via knowledge distillation, due to computational constraints. Edit the model name here to use a different model.\n",
        "\n",
        "## About this model\n",
        "\n",
        "DistilBERT has 66 million parameters, 6 layers, and 12 attention heads. It ignores case, like the \"teacher\" BERT model.\n",
        "\n",
        "More about DistilBERT:\n",
        "\n",
        "https://huggingface.co/distilbert/distilbert-base-uncased\n",
        "\n",
        "https://huggingface.co/docs/transformers/en/model_doc/distilbert"
      ],
      "metadata": {
        "id": "aZiDWA51EoVw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "name = \"distilbert-base-uncased\""
      ],
      "metadata": {
        "id": "qIa7VMBnErAk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading data\n",
        "\n",
        "The data was tokenized and saved (in case of disconnected runtime) using this script:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "from datasets import load_dataset\n",
        "amazon_reviews=load_dataset(\"amazon_polarity\")\n",
        "def tokenize_function(batch):\n",
        "    return tokenizer(\n",
        "        batch[\"content\"], # content col contains actual review text\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",   # keeping batch-length uniform\n",
        "        max_length=128,         # reasonable for seq class.\n",
        "        )\n",
        "\n",
        " tokenized_reviews = amazon_reviews.map(tokenize_function, batched=True,num_proc=8) # using parallelization to speed up tokenization\n",
        "\n",
        " tokenized_reviews.save_to_disk(\"./tokenized_amazon\")\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "dJTUMpQ_Cjvp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# loading tokenized data\n",
        "tokenized_reviews = load_from_disk(\"./tokenized_amazon\")"
      ],
      "metadata": {
        "id": "wJqTYcMoOB_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the model-compatible tokenizer from HuggingFace\n",
        "tokenizer = AutoTokenizer.from_pretrained(name)\n",
        "\n",
        "# load the data collator (creates uniform batches) with the tokenizer\n",
        "# (preserves compatible padding tokens, attention mask rules, etc)\n",
        "collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "vbW0Vlx3EeWi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Subsampling\n",
        "\n",
        "Run this cell to use less data and edit the train/eval dataset names per the comments in the training script.\n",
        "\n",
        "This script was not used to finetune the provided model."
      ],
      "metadata": {
        "id": "N68zq0iAMcRt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# take random subset of tokenized data (full dataset takes ~ 10h/epoch on T4 GPU available in Colab)\n",
        "\n",
        "ratio = 0.1\n",
        "\n",
        "len_subsample_train = int(ratio * len(tokenized_reviews[\"train\"]))\n",
        "len_subsample_test = int(ratio * len(tokenized_reviews[\"test\"]))\n",
        "\n",
        "print(\"The new train set will have \", len_subsample_train, \"samples\")\n",
        "print(\"The new test set will have \", len_subsample_test, \"samples\")\n",
        "\n",
        "train = tokenized_reviews[\"train\"].shuffle(seed=7)[:len_subsample_train]\n",
        "eval = tokenized_reviews[\"test\"].shuffle(seed=7)[:len_subsample_test]"
      ],
      "metadata": {
        "id": "pGanHB6iyrb4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare model for LoRA Fine-Tuning"
      ],
      "metadata": {
        "id": "hN7Or42qOjph"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# loading the model\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(name)\n",
        "\n",
        "# The output below may say something like \"some weights were not initialized\"\n",
        "# This indicates that the task-specific head was randomly initialized, as the model loaded\n",
        "# from checkpoint was trained for a different task.\n",
        "# This may lead to worse performance downstream, but it does not mean the entire model is trained from scratch"
      ],
      "metadata": {
        "id": "jm0e8b8kfZ7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    init_lora_weights=\"gaussian\",\n",
        "    target_modules=[\"q_lin\", \"v_lin\"]\n",
        ")"
      ],
      "metadata": {
        "id": "RwtqJz4Hf9hK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "peft_model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Uncomment and run the code below to see how much the number of trainable params is reduced!!\n",
        "# peft_model.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "EcVbE-6mgMYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-tuning!!"
      ],
      "metadata": {
        "id": "Ae90muZLPI8a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoints_dir = \"./lora_results\"\n",
        "\n",
        "# check if checkpoints exist to determine if we need to load them, rather than re-fine-tuning\n",
        "# checkpoints directory must exist and contain checkpoints\n",
        "has_checkpoints = os.path.exists(checkpoints_dir) and len(os.listdir(checkpoints_dir))>0"
      ],
      "metadata": {
        "id": "2_pUbXl9zgIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load adapters and inject into pretrained model\n",
        "finetuned_model = peft_model.merge_and_unload()\n",
        "\n",
        "# save for later use + in case of disconnected runtime\n",
        "finetuned_model.save_pretrained(\"merged_distilbert_amazon\")"
      ],
      "metadata": {
        "id": "aerJtv7gOyss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    # set model args\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=checkpoints_dir,\n",
        "        learning_rate=1e-3, # increased from 2e-4 to 1e-3 due to loss plateau at about 0.2\n",
        "        num_train_epochs=1, # originally set to one because the model used is relatively small\n",
        "        per_device_train_batch_size=4 # batch size was chosen heuristically, you may want to change it (+ LR acc.)\n",
        "    )\n",
        "\n",
        "    # init trainer\n",
        "    trainer = Trainer(\n",
        "        model=peft_model, # fine-tuning the low-rank adapters only\n",
        "        args=training_args,\n",
        "        train_dataset= tokenized_reviews[\"train\"], # train if ran subsampling script\n",
        "        eval_dataset= tokenized_reviews[\"test\"],  # eval ' ' ' '\n",
        "        tokenizer=tokenizer, # already tokenized, HF asks for this because metadata is used during training\n",
        "        data_collator=collator\n",
        "    )\n",
        "\n",
        "    # train and resume from checkpoint if available\n",
        "    trainer.train(resume_from_checkpoint=has_checkpoints)"
      ],
      "metadata": {
        "id": "ZkzP9gS-gTTq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving Adapters"
      ],
      "metadata": {
        "id": "uQGGeoPHNpg3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "peft_model.save_pretrained(\"distilbert_amazon_adapters\")"
      ],
      "metadata": {
        "id": "DgtUay01goHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference Example:"
      ],
      "metadata": {
        "id": "SAWswr6RM4aY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace with whatever you want. Then run the cell below!\n",
        "pos_prompt = \"This is a very good laptop. It runs my model without crashing. Five stars\"\n",
        "neg_prompt = \"This is a terrible laptop. It crashes constantly. Would give zero stars if I could\"\n",
        "\n",
        "# Failure case - the model cannot detect sarcasm - possibly due to untrained classifier head,\n",
        "# short training, and/or small model size -> even smaller adapters\n",
        "# Could also be a lack of domain-specific knowledge, since the fine-tuning dataset is not specific to tech products\n",
        "sarcasm = \"I love the way this laptop crashes constantly. I can run VS code for a whole second. Exactly what I was looking for\""
      ],
      "metadata": {
        "id": "dDpUX_UCNFzC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# change pos_prompt to whatever prompt you want to use\n",
        "# inputs tokenized and moved to gpu for inference\n",
        "inputs = tokenizer(pos_prompt, return_tensors=\"pt\").to(finetuned_model.device)\n",
        "\n",
        "# get model output, logits, prediction, prob tensor\n",
        "with torch.no_grad():\n",
        "  out = model(**inputs)\n",
        "  logits=out.logits\n",
        "  predicted_class = logits.argmax(dim=-1)\n",
        "  probs=torch.softmax(logits,dim=-1)\n",
        "\n",
        "# small function to make outputs more human-readable\n",
        "def polarity(pred):\n",
        "  return \"Positive\" if pred[0] else \"Negative\"\n",
        "\n",
        "# print results!!\n",
        "print(\"Review Polarity: \", polarity(predicted_class))\n",
        "print(\"Probabilities:\", probs)\n",
        "print(\"Raw logits:\", logits)"
      ],
      "metadata": {
        "id": "w7JWyB9S-od7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}